{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3r1xWLWcDQ6"
   },
   "source": [
    "Pontificia Universidad Católica de Chile <br>\n",
    "Departamento de Ciencia de la Computación <br>\n",
    "IIC2433 - Minería de Datos\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <h2> Tarea 5 - Autoencoders </h2>\n",
    "    <h1>   </h1>\n",
    "    <p>\n",
    "        Profesor Marcelo Mendoza<br>\n",
    "        Segundo Semestre 2022<br>    \n",
    "        Fecha de entrega: 25 de noviembre\n",
    "    </p>\n",
    "    <br>\n",
    "</center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du5vX2PwcL4c"
   },
   "source": [
    "## Indicaciones\n",
    "\n",
    "Deberás entregar **SOLO** el archivo .ipynb en el buzón respectivo en canvas. \n",
    "\n",
    "**IMPORTANTE**: \n",
    "- Se te dará puntaje tanto por código como por la manera en la que respondas las preguntas planteadas. Es decir, si tienes un código perfecto pero este no es explicado o no se responden preguntas asociadas a este, no se tendrá el puntaje completo.\n",
    "- El notebook debe tener todas las celdas de código ejecutadas. Cualquier notebook que no las tenga no podrá ser corregido.\n",
    "- El carácter de esta tarea es **INDIVIDUAL**. Cualquier instancia de copia resultará en un 1,1 como nota de curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJIQGOXvdIvm"
   },
   "source": [
    "#1. Obtención del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1jNgXu3dMZ0"
   },
   "source": [
    "Para esta tarea trabajaremos con el dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist). Este contiene múltiples imágenes de artículo de ropa representados como un dato con 28x28 pixeles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UCtI1ENOdldi"
   },
   "outputs": [],
   "source": [
    "#Librerías que les podrían ser útiles\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTHfLttnyPzC"
   },
   "source": [
    "El dataset es parte de la librería de keras, por lo que no necesitan descargarlo. A continuación esta el código para obtenerlo. También para efectos de esta tarea solo trabajaremos con un subset de los datos, para reducir los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fcFI-0nZ3CmK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X = np.concatenate([x_train, x_test], axis=0)\n",
    "Y = np.concatenate([y_train, y_test], axis=0)\n",
    "_, X, _, Y = train_test_split(X, Y, test_size=0.2, random_state=23)\n",
    "X_matrix = np.expand_dims(X, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4npUf8N6crLx"
   },
   "source": [
    "#2. Reducción de dimensionalidad (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJHmIazgzdri"
   },
   "source": [
    "En esta parte usaremos dos métodos diferentes para reducir las dimensionalidades del dataset. Para ambos casos se hará una reducción a 6 dimensionalidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPDZW5Ml8ODj"
   },
   "source": [
    "Para que los datos sean compatibles con el modelo es necesario transformar de una matriz de 28x28 a una fila con todos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PzOVASCa5Iad"
   },
   "outputs": [],
   "source": [
    "X_flat = []\n",
    "for i in range(X.shape[0]):\n",
    "    X_flat.append(X[i].flatten())\n",
    "X_flat = np.array(X_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7ONZvVFzBUJ"
   },
   "source": [
    "### 2.0 Normalizar (0.25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a23oYXizJCi"
   },
   "source": [
    "Para que las capas funcionen mejor, es necesario normalizar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fbIrN45AzIaD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Normaliza\n",
    "X_flat_df = pd.DataFrame(X_flat)\n",
    "X_df_n = scaler.fit_transform(X_flat_df)\n",
    "\n",
    "X_df_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di59t973cxTn"
   },
   "source": [
    "##2.1 PCA (0.75 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GKj62_azvRg"
   },
   "source": [
    "Primero debes hacer un PCA de 6 dimensiones. Y luego usar `cross_val_score`, con `cv=5` para medir el `accuracy` promedio de la reducción en una predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "DiyvFFMWdriz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.72610137 -4.03377579  0.85600161 -0.36006788  0.50919045  0.38765597]\n",
      " [ 4.28380382 -4.06065227  2.69805022 -0.10023676  0.07208649  1.50045908]\n",
      " [-3.38741289 -0.15876603 -3.66314479  1.79730453 -0.57426369 -0.7864482 ]\n",
      " ...\n",
      " [ 5.19428138 -1.79200798  1.39094472 -2.08765819  1.53335431 -0.64107263]\n",
      " [-2.04807435 -2.57483705 -2.01499749  0.70129716 -0.40156554  0.11188742]\n",
      " [ 0.36500625 -1.04644345 -0.71341361 -1.47716135  0.18000252  0.39106875]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14000, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Realiza el PCA\n",
    "pca_6 = PCA(n_components=6)\n",
    "X_pca = pca_6.fit_transform(X_df_n)\n",
    "print(X_pca)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculamos la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29009527 0.17671623 0.059695   0.04983527 0.03840709 0.03480205]\n",
      "0.10825848311100379\n"
     ]
    }
   ],
   "source": [
    "print(pca_6.explained_variance_ratio_)\n",
    "print(np.mean(pca_6.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKUhYOls0U6h"
   },
   "source": [
    "Para clasificar usaremos un modelo de Random Forest, el cual luego se debe usar para medir el `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k0b0uQwpdw3a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MVr_4Aq2g6fE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75964286 0.75107143 0.75928571 0.75785714 0.75035714]\n",
      "0.7556428571428572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#Realiza la prediccion\n",
    "accuracy = cross_val_score(rfc, X_pca, Y, cv=5)\n",
    "print(accuracy)\n",
    "print(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbrPwRQocyb0"
   },
   "source": [
    "##2.2 Autoencoders (1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3caeW2KH0m3t"
   },
   "source": [
    "Ahora debes usar autoencoders para reducir a 6 dimensiones. \n",
    "Para esta parte te pedimos que alcances por lo menos un **0.04** de accuracy por sobre el obtenido en PCA utilizando `cross_val_score` de la misma forma. Para lograr esto puedes cambiar los hiperparámetros en diferentes partes, agregar capas, etc. Es importante notar que `X_train` y `X_test` definidos a continuación son para ser usados en el entrenamiento del autoencoder, para la predicción se sigue teniendo que usar el X estandarizado antes obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "dBKBw0Hm0_cQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(X_df_n, test_size=0.3, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 784)\n",
      "(9800, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usó https://pythonprogramming.net/autoencoders-tutorial/, https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 240)               188400    \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 64)                15424     \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 64)                448       \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 240)               15600     \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 784)               188944    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 409,206\n",
      "Trainable params: 409,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size_1 = 240\n",
    "hidden_size_2 = 64\n",
    "code_size = 6\n",
    "\n",
    "x = keras.layers.Input(shape=(input_size,))\n",
    "\n",
    "# Encoder\n",
    "hidden_1 = Dense(hidden_size_1, activation='selu')(x)\n",
    "hidden_2 = Dense(hidden_size_2, activation='selu')(hidden_1)\n",
    "h = Dense(code_size, activation='selu')(hidden_2)\n",
    "\n",
    "# Decoder\n",
    "hidden_3 = Dense(hidden_size_2, activation='selu')(h)\n",
    "hidden_4 = Dense(hidden_size_1, activation='selu')(hidden_3)\n",
    "r = Dense(input_size, activation='sigmoid')(hidden_4)\n",
    "\n",
    "autoencoder = keras.Model(x, r, name=\"autoencoder\")\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "39/39 [==============================] - 2s 28ms/step - loss: 0.0855 - val_loss: 0.0461\n",
      "Epoch 2/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0408 - val_loss: 0.0388\n",
      "Epoch 3/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0357 - val_loss: 0.0350\n",
      "Epoch 4/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0331 - val_loss: 0.0332\n",
      "Epoch 5/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0314 - val_loss: 0.0313\n",
      "Epoch 6/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0291 - val_loss: 0.0285\n",
      "Epoch 7/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0272 - val_loss: 0.0271\n",
      "Epoch 8/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0262 - val_loss: 0.0264\n",
      "Epoch 9/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0255 - val_loss: 0.0258\n",
      "Epoch 10/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0250 - val_loss: 0.0253\n",
      "Epoch 11/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0246 - val_loss: 0.0249\n",
      "Epoch 12/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0243 - val_loss: 0.0245\n",
      "Epoch 13/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0238 - val_loss: 0.0242\n",
      "Epoch 14/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0233 - val_loss: 0.0235\n",
      "Epoch 15/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0228 - val_loss: 0.0232\n",
      "Epoch 16/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0225 - val_loss: 0.0229\n",
      "Epoch 17/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0221 - val_loss: 0.0226\n",
      "Epoch 18/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0220 - val_loss: 0.0225\n",
      "Epoch 19/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0217 - val_loss: 0.0221\n",
      "Epoch 20/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0214 - val_loss: 0.0220\n",
      "Epoch 21/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0212 - val_loss: 0.0217\n",
      "Epoch 22/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0211 - val_loss: 0.0215\n",
      "Epoch 23/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0209 - val_loss: 0.0214\n",
      "Epoch 24/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0207 - val_loss: 0.0213\n",
      "Epoch 25/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0206 - val_loss: 0.0211\n",
      "Epoch 26/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0204 - val_loss: 0.0210\n",
      "Epoch 27/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0203 - val_loss: 0.0210\n",
      "Epoch 28/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0203 - val_loss: 0.0208\n",
      "Epoch 29/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0201 - val_loss: 0.0207\n",
      "Epoch 30/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0199 - val_loss: 0.0206\n",
      "Epoch 31/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0198 - val_loss: 0.0205\n",
      "Epoch 32/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0198 - val_loss: 0.0205\n",
      "Epoch 33/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0197 - val_loss: 0.0202\n",
      "Epoch 34/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0195 - val_loss: 0.0201\n",
      "Epoch 35/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0194 - val_loss: 0.0202\n",
      "Epoch 36/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0193 - val_loss: 0.0200\n",
      "Epoch 37/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0192 - val_loss: 0.0197\n",
      "Epoch 38/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0190 - val_loss: 0.0195\n",
      "Epoch 39/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0189 - val_loss: 0.0196\n",
      "Epoch 40/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 41/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0186 - val_loss: 0.0193\n",
      "Epoch 42/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 43/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0183 - val_loss: 0.0194\n",
      "Epoch 44/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 45/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0182 - val_loss: 0.0189\n",
      "Epoch 46/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0181 - val_loss: 0.0189\n",
      "Epoch 47/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 48/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0179 - val_loss: 0.0187\n",
      "Epoch 49/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0178 - val_loss: 0.0186\n",
      "Epoch 50/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0178 - val_loss: 0.0186\n",
      "Epoch 51/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0178 - val_loss: 0.0185\n",
      "Epoch 52/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0177 - val_loss: 0.0185\n",
      "Epoch 53/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0177 - val_loss: 0.0185\n",
      "Epoch 54/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0176 - val_loss: 0.0184\n",
      "Epoch 55/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0175 - val_loss: 0.0184\n",
      "Epoch 56/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0174 - val_loss: 0.0183\n",
      "Epoch 57/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0174 - val_loss: 0.0183\n",
      "Epoch 58/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0173 - val_loss: 0.0182\n",
      "Epoch 59/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0173 - val_loss: 0.0182\n",
      "Epoch 60/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0172 - val_loss: 0.0181\n",
      "Epoch 61/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0172 - val_loss: 0.0180\n",
      "Epoch 62/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 63/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 64/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0171 - val_loss: 0.0181\n",
      "Epoch 65/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0170 - val_loss: 0.0179\n",
      "Epoch 66/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0169 - val_loss: 0.0178\n",
      "Epoch 67/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0169 - val_loss: 0.0178\n",
      "Epoch 68/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0169 - val_loss: 0.0178\n",
      "Epoch 69/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0168 - val_loss: 0.0178\n",
      "Epoch 70/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0168 - val_loss: 0.0177\n",
      "Epoch 71/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0168 - val_loss: 0.0177\n",
      "Epoch 72/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0167 - val_loss: 0.0178\n",
      "Epoch 73/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0167 - val_loss: 0.0177\n",
      "Epoch 74/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0166 - val_loss: 0.0176\n",
      "Epoch 75/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0166 - val_loss: 0.0178\n",
      "Epoch 76/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0166 - val_loss: 0.0176\n",
      "Epoch 77/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 78/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0166 - val_loss: 0.0177\n",
      "Epoch 79/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0165 - val_loss: 0.0175\n",
      "Epoch 80/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0164 - val_loss: 0.0175\n",
      "Epoch 81/150\n",
      "39/39 [==============================] - 1s 30ms/step - loss: 0.0164 - val_loss: 0.0175\n",
      "Epoch 82/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0164 - val_loss: 0.0174\n",
      "Epoch 83/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 84/150\n",
      "39/39 [==============================] - 1s 24ms/step - loss: 0.0163 - val_loss: 0.0175\n",
      "Epoch 85/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 86/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0163 - val_loss: 0.0174\n",
      "Epoch 87/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 88/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 89/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0162 - val_loss: 0.0175\n",
      "Epoch 90/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 91/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0162 - val_loss: 0.0173\n",
      "Epoch 92/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0161 - val_loss: 0.0173\n",
      "Epoch 93/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0161 - val_loss: 0.0173\n",
      "Epoch 94/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0160 - val_loss: 0.0172\n",
      "Epoch 95/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0160 - val_loss: 0.0172\n",
      "Epoch 96/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0160 - val_loss: 0.0171\n",
      "Epoch 97/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0160 - val_loss: 0.0171\n",
      "Epoch 98/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 99/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 100/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 101/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 102/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0159 - val_loss: 0.0173\n",
      "Epoch 103/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 104/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 105/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0159 - val_loss: 0.0171\n",
      "Epoch 106/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0158 - val_loss: 0.0171\n",
      "Epoch 107/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 108/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 109/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0157 - val_loss: 0.0171\n",
      "Epoch 110/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 111/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0156 - val_loss: 0.0170\n",
      "Epoch 112/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.0171\n",
      "Epoch 113/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.0170\n",
      "Epoch 114/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0156 - val_loss: 0.0170\n",
      "Epoch 115/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0156 - val_loss: 0.0171\n",
      "Epoch 116/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0156 - val_loss: 0.0171\n",
      "Epoch 117/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0156 - val_loss: 0.0169\n",
      "Epoch 118/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0155 - val_loss: 0.0170\n",
      "Epoch 119/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 120/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 121/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0155 - val_loss: 0.0168\n",
      "Epoch 122/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 123/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0155 - val_loss: 0.0169\n",
      "Epoch 124/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0154 - val_loss: 0.0169\n",
      "Epoch 125/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0154 - val_loss: 0.0169\n",
      "Epoch 126/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0154 - val_loss: 0.0169\n",
      "Epoch 127/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0154 - val_loss: 0.0168\n",
      "Epoch 128/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.0169\n",
      "Epoch 129/150\n",
      "39/39 [==============================] - 1s 25ms/step - loss: 0.0154 - val_loss: 0.0170\n",
      "Epoch 130/150\n",
      "39/39 [==============================] - 1s 26ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 131/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.0169\n",
      "Epoch 132/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 133/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 134/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.0168\n",
      "Epoch 135/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0153 - val_loss: 0.0169\n",
      "Epoch 136/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0152 - val_loss: 0.0168\n",
      "Epoch 137/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0152 - val_loss: 0.0168\n",
      "Epoch 138/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0152 - val_loss: 0.0168\n",
      "Epoch 139/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0152 - val_loss: 0.0167\n",
      "Epoch 140/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0152 - val_loss: 0.0168\n",
      "Epoch 141/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0152 - val_loss: 0.0167\n",
      "Epoch 142/150\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.0152 - val_loss: 0.0169\n",
      "Epoch 143/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0169\n",
      "Epoch 144/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0168\n",
      "Epoch 145/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0167\n",
      "Epoch 146/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0167\n",
      "Epoch 147/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0150 - val_loss: 0.0167\n",
      "Epoch 148/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0167\n",
      "Epoch 149/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0150 - val_loss: 0.0168\n",
      "Epoch 150/150\n",
      "39/39 [==============================] - 1s 23ms/step - loss: 0.0151 - val_loss: 0.0167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249ab5f8bc8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train, epochs=150, batch_size=256, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "prediccion = autoencoder.predict(X_df_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "udCDKyuD1d7k"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "WPgyVOtWfpNA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.825      0.8175     0.8325     0.82285714 0.81964286]\n",
      "0.8234999999999999\n"
     ]
    }
   ],
   "source": [
    "#Usa cross_val_score para predecir con rfc\n",
    "accuracy = cross_val_score(rfc, prediccion, Y , cv=5)\n",
    "print(accuracy)\n",
    "print(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el accuracy es más de 0.04 mejor que en PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tqKsLzNOlkR"
   },
   "source": [
    "##2.3 Preguntas (1 pto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTDx2d62OswO"
   },
   "source": [
    "1. ¿Por qué el Autoencoder tiene un encoder y decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynH7WsVeO7aX"
   },
   "source": [
    "El encoder realiza la función de comprimir las variables y reducir la dimensionalidad, mientras que el decoder toma los datos con la dimensionalidad reducida y los vuelve a dejar en la dimensionalidad inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ4gvJ1bPPY2"
   },
   "source": [
    "2. ¿Por qué es posible obtener mejores resultados usando Autoencoders por sobre PCA para reducir dimensiones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q095nobYP9eL"
   },
   "source": [
    "Se utilizó https://towardsdatascience.com/autoencoders-vs-pca-when-to-use-which-73de063f5d7\n",
    "\n",
    "PCA es escencialmente una transformación lineal, en cambio autoencoder es capaz de modelar funciones no lineares complejas. Otra diferencia es que al ser entrenado, el autoencoder mantiene cierta correlación que PCA no. El hecho de que se pueda agregar layers al autoencoder le agrega complejidad y certeza, cosa que no puede hacer el PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1LBOA9PcuHK"
   },
   "source": [
    "#3. Autoencoder variacional (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tlRF34_HdB8"
   },
   "source": [
    "Los autoencoders variacionales (VAEs), son una potente herramienta que permite incorporar el uso de estadísticas en autoencoders tradicionales, y permiten identificar de muy buena forma dimensiones latentes en un dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLK5bph7IDVR"
   },
   "source": [
    "Tu tarea en esta sección es implementar un VAE que trabaje sobre el mismo dataset de imagenes de ropa de la parte anterior. Para esto, es fuertemente recomendado que utilices la documentación de [Keras](https://keras.io/) como guía para la implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZH_-_f_VrfN"
   },
   "source": [
    "Disclaimer: Es posible que tu código en esta parte se demore en correr, pero debería tomar a lo más 1 hora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klbnd-nXSHsX"
   },
   "source": [
    "##3.1 VAE (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó https://towardsdatascience.com/variational-autoencoders-as-generative-models-with-keras-e0c79415a7eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "xMcE4ldtis84"
   },
   "outputs": [],
   "source": [
    "#Encoder\n",
    "\n",
    "input_data = keras.layers.Input(shape=(28, 28, 1))\n",
    "encoder = keras.layers.Conv2D(64, (5,5), activation='relu')(input_data)\n",
    "encoder = keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "encoder = keras.layers.Conv2D(64, (3,3), activation='relu')(encoder)\n",
    "encoder = keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "encoder = keras.layers.Conv2D(32, (3,3), activation='relu')(encoder)\n",
    "encoder = keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "encoder = keras.layers.Flatten()(encoder)\n",
    "encoder = keras.layers.Dense(6)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_features(distribution):\n",
    "    distribution_mean, distribution_variance = distribution\n",
    "    batch_size = tf.shape(distribution_variance)[0]\n",
    "    random = keras.backend.random_normal(shape=(batch_size, tf.shape(distribution_variance)[1]))\n",
    "    return distribution_mean + tf.exp(0.5 * distribution_variance) * random\n",
    " \n",
    "distribution_mean = keras.layers.Dense(2, name='mean')(encoder)\n",
    "distribution_variance = keras.layers.Dense(2, name='log_variance')(encoder)\n",
    "latent_encoding = keras.layers.Lambda(sample_latent_features)([distribution_mean, distribution_variance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 24, 24, 64)   1664        ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 12, 12, 64)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 10, 10, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 64)    0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 3, 3, 32)     18464       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 32)           0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_96 (Dense)               (None, 6)            198         ['flatten_14[0][0]']             \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 2)            14          ['dense_96[0][0]']               \n",
      "                                                                                                  \n",
      " log_variance (Dense)           (None, 2)            14          ['dense_96[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 2)            0           ['mean[0][0]',                   \n",
      "                                                                  'log_variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 57,282\n",
      "Trainable params: 57,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = tf.keras.Model(input_data, latent_encoding)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "eEkX0wywi4xE"
   },
   "outputs": [],
   "source": [
    "#Decoder\n",
    "decoder_input = keras.layers.Input(shape=(2))\n",
    "decoder = keras.layers.Dense(64)(decoder_input)\n",
    "decoder = keras.layers.Reshape((1, 1, 64))(decoder)\n",
    "decoder = keras.layers.Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    " \n",
    "decoder = keras.layers.Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = keras.layers.UpSampling2D((2,2))(decoder)\n",
    " \n",
    "decoder = keras.layers.Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = keras.layers.UpSampling2D((2,2))(decoder)\n",
    " \n",
    "decoder_output = keras.layers.Conv2DTranspose(1, (5,5), activation='relu')(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 64)                192       \n",
      "                                                                 \n",
      " reshape_22 (Reshape)        (None, 1, 1, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 3, 3, 64)         36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 5, 5, 64)         36928     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 10, 10, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 12, 12, 64)       36928     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 1)        1601      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,577\n",
      "Trainable params: 112,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = keras.Model(decoder_input, decoder_output)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "Bu_lLl91i9eG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 2)                 57282     \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 28, 28, 1)         112577    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 169,859\n",
      "Trainable params: 169,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VAE    \n",
    "encoded = encoder_model(input_data)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder = keras.models.Model(input_data, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmCk6_qOVEgD"
   },
   "source": [
    "Para esta parte te recomendamos usar los datos en la forma `X_matrix` definida en la obtención del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "BjyFifObjCF3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "219/219 [==============================] - 39s 169ms/step - loss: 27.8861 - val_loss: 13427.7100\n",
      "Epoch 2/2\n",
      "219/219 [==============================] - 38s 174ms/step - loss: 27.8666 - val_loss: 13427.7168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249b4bc4d08>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenamiento VAE\n",
    "autoencoder.fit(X, Y, epochs=2, batch_size=64, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNAzUD5GIzfZ"
   },
   "source": [
    "Ahora que tienes un VAE entrenado para este dataset, muestra su espacio latente correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "x83YNt-1rN2v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 28, 28, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_21'), name='input_21', description=\"created by layer 'input_21'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 251, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer 'model_1' (type Functional).\n    \n    Input 0 of layer \"conv2d_2\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'model_1' (type Functional):\n      • inputs=tf.Tensor(shape=(None,), dtype=uint8)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-4c85d406b618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\francisco\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 251, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer 'model_1' (type Functional).\n    \n    Input 0 of layer \"conv2d_2\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'model_1' (type Functional):\n      • inputs=tf.Tensor(shape=(None,), dtype=uint8)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "#Espacio latente\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for i in range(10000):\n",
    "    z.append(y_test[i])\n",
    "    op = encoder_model.predict(np.array([Y[i]]))\n",
    "    x.append(op[0][0])\n",
    "    y.append(op[0][1])\n",
    "df = pd.DataFrame()\n",
    "df['x'] = x\n",
    "df['y'] = y\n",
    "df['z'] = [\"digit-\"+str(k) for k in z]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='x', y='y', hue='z', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYskPJ7bQQy2"
   },
   "source": [
    "##3.2 Preguntas teóricas (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiVBnjNUQVRy"
   },
   "source": [
    "1. ¿En qué parte del código se puede encontrar la diferencia principal entre un AE tradicional y VAE? (Copia y pega la parte del código para explicar mejor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU3VswgDQlZ2"
   },
   "source": [
    "Se puede ver que al momento de definir las layers hay otras funciones tales como Conv2DTranspose, UpSampling2D para el decode y Conv2D, MaxPooling2D para el encode. Además se crea la siguiente funcion:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sample_latent_features(distribution):\n",
    "    distribution_mean, distribution_variance = distribution\n",
    "    batch_size = tf.shape(distribution_variance)[0]\n",
    "    random = keras.backend.random_normal(shape=(batch_size, tf.shape(distribution_variance)[1]))\n",
    "    return distribution_mean + tf.exp(0.5 * distribution_variance) * random\n",
    " \n",
    "distribution_mean = keras.layers.Dense(2, name='mean')(encoder)\n",
    "distribution_variance = keras.layers.Dense(2, name='log_variance')(encoder)\n",
    "latent_encoding = keras.layers.Lambda(sample_latent_features)([distribution_mean, distribution_variance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk33TxOyQm4z"
   },
   "source": [
    "2. Explica qué hace la parte del código de la pregunta anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seC6Q7itRRqN"
   },
   "source": [
    "Esta función completa el encoder sacando el promedio y la log-varianza para para formar un latent encoding vector. Este es pasado al decoder con el fin de reconstruir la data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWRaH_kSSonI"
   },
   "source": [
    "3. ¿En qué situación puede ser útil utilizar VAE y por qué? (Da 2 ejemplos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m9-Z_agTDH4"
   },
   "source": [
    "Se uso https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8774760/\n",
    "\n",
    "VAE es util cuando la data que uno tiene debe ser reconstruida lo más cercano a lo original. Algunos ejemplos son generar data para audios, textos e imagenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TEjrjCoTMOp"
   },
   "source": [
    "4. ¿Cúal es una posible desventaja del método VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtAQ3qFSTTex"
   },
   "source": [
    "Toma mucho tiempo y recursos computacionales, Por lo que si el problema puede ser resuelto con AE mejor que se resuelva con eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
