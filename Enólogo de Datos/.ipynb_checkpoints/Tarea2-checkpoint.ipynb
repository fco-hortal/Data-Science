{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnDiLwUdS1Gu"
   },
   "source": [
    "Pontificia Universidad Católica de Chile <br>\n",
    "Departamento de Ciencia de la Computación <br>\n",
    "IIC2433 - Minería de Datos\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <h2> Tarea 2 </h2>\n",
    "    <h1> Ham o spam  </h1>\n",
    "    <p>\n",
    "        Profesor Marcelo Mendoza<br>\n",
    "        Segundo Semestre 2022<br>    \n",
    "        Fecha de entrega: Viernes 23 de septiembre 22.00 horas\n",
    "    </p>\n",
    "    <br>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXr7-famVI0x"
   },
   "source": [
    "## Indicaciones\n",
    "\n",
    "Deberás entregar **SOLO** el archivo .ipynb en el buzón respectivo en canvas. \n",
    "\n",
    "**IMPORTANTE**: \n",
    "- Se te dará puntaje tanto por código como por la manera en la que respondas las preguntas planteadas. Es decir, si tienes un código perfecto pero este no es explicado o no se responden preguntas asociadas a este, no se tendrá el puntaje completo.\n",
    "- El notebook debe tener todas las celdas de código ejecutadas. Cualquier notebook que no las tenga no podrá ser corregido.\n",
    "- El carácter de esta tarea es **INDIVIDUAL**. Cualquier instancia de copia resultará en un 1,1 como nota de curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEM29b-HVfIi"
   },
   "source": [
    "Utilizaremos una base de datos ubicada en Kaggle https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset que puedes encontrar igualmente en canvas como csv para descargar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATIl8_5SZqH1"
   },
   "source": [
    "## Introducción\n",
    "\n",
    "Nadie es inmune a recibir mensajes de Movistar o Entel ofreciéndonos planes. Abrir un mensaje para encontrarse con una hermosa sorpresa: es spam. \n",
    "\n",
    "Este es un problema a nivel mundial, tanto así que se han armado bases de datos con diferentes mensajes de texto recibidos por persona y si son considerados como spam o no (si no son spam se refiere a los mensajes como ham).\n",
    "\n",
    "Utilizando la vectorización de frases y clusterizando estas, deberás predecir si esta es o no spam. Además, deberás obtener los índices de calidad de los clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HanOLeY9fjFV"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IResCwZ4gp8H"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip3 uninstall spacy\n",
    "!pip3 install spacy\n",
    "\n",
    "!spacy download en_core_web_lg\n",
    "clear_output()\n",
    "print('Ahora debes reiniciar el entorno de ejecución y ejecutar a partir de la siguiente celda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p4Ad0YDg0iH"
   },
   "source": [
    "## 1. Importar librerías y descargar dataset\n",
    "En esta tarea trabajaremos con la librería `spacy` y el pipeline `en_core_web_lg` el cual pesa más de 500 MB y contiene un vocabulario en inglés de más de medio millón de palabras. Cada una de estas palabras es representable a partir de un vector de 300 dimensiones que nos ayudarán en la tarea. Revisa la [documentación](https://spacy.io/api) documentación de la librería para saber más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPg3sNQOVH_5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B25-rStYiGjf"
   },
   "source": [
    "### Leer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIX2mob7S0HQ"
   },
   "outputs": [],
   "source": [
    "url = 'spam.csv'\n",
    "df = pd.read_csv(url, index_col=0, encoding=\"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTPNejOsifeV"
   },
   "source": [
    "## 2. Procesamiento de los datos (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q04il_DkYTx"
   },
   "source": [
    "### 2.1 Eliminación de datos\n",
    "Solamente analizaremos las columnas de si es o no spam y cuál es el mensaje. Elimina las columnas restantes y preprocesa las filas eliminando los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JydU6M3ekp7U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRc8KlOrKWya"
   },
   "source": [
    "### 2.2 Preprocesamiento de oraciones\n",
    "\n",
    "Acá te damos el código para preprocesar un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrEFtXFTlLHY"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "def remove_punctuation(text):\n",
    "  text = [token for token in text if not token.is_punct]\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "  words = [word for word in words if not word in all_stopwords]\n",
    "  return words\n",
    "\n",
    "def lemmatize(words):\n",
    "  words = [word.lemma_ for word in words]\n",
    "  return words\n",
    "\n",
    "def remove_non_alpha(words):\n",
    "  words = [word for word in words if word.isalpha()]\n",
    "  return words\n",
    "\n",
    "def lower(words):\n",
    "  words = [word.lower() for word in words]\n",
    "  return words\n",
    "\n",
    "def min_len(words, length=3):\n",
    "  words = [word for word in words if len(word)>=length]\n",
    "  return words\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "  doc = nlp(text)\n",
    "  tokens = remove_punctuation(doc)\n",
    "  tokens = remove_stopwords(tokens)\n",
    "  tokens = lemmatize(tokens)\n",
    "  tokens = remove_non_alpha(tokens)\n",
    "  tokens = lower(tokens)\n",
    "  tokens = min_len(tokens, length=3)\n",
    "\n",
    "  return ' '.join(tokens).strip()\n",
    "\n",
    "# Este es un ejemplo para que veas si tu preprocesamiento funcionó.\n",
    "new_text = preprocess(\"This is the 2nd time we have tried 2 contact...\")\n",
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aN3CB3ijwlB"
   },
   "source": [
    "Preprocesa todos los mensajes utilizando el método preprocess y guárdalos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8R2tnE1PdxCE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wga_PgMmMmr"
   },
   "source": [
    "### 2.3 Vectorizar oraciones\n",
    "En esta tarea, el vector de una oración será el promedio de los vectores de cada una de las palabras que fueron preprocesadas de la oración. La función presentada a continuación vectoriza una oración a partir de los vectores de las palabras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z369fruNmVMF"
   },
   "outputs": [],
   "source": [
    "def sentence_vector(text):\n",
    "  text = nlp(text)\n",
    "  vectores = []\n",
    "  for t in text:\n",
    "    t_vector = t.vector\n",
    "    vectores.append(t_vector)\n",
    "  return np.array(vectores).sum(axis=0)/len(vectores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aUAuR0_g195"
   },
   "source": [
    "### 2.4. Obtener matriz de distancias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbHlE8PjhKNg"
   },
   "source": [
    "Obtén una forma de calcular una matriz que por cada par distintos de oraciones contenga la distancia euclidiana y coseno entre los vectores que representan a cada una. \n",
    "\n",
    "Hint: el método pairwise_distances de sklearn realiza esta operación eficientemente y no genera problemas de RAM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBSyhBgshK4o"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NGGrYDXe_Vb"
   },
   "source": [
    "## 3. Clase AgglomerativeClustering (3 puntos)\n",
    "\n",
    "Esta clase debe implemetar el algoritmo de clustering jerárquico aglomerativo. Para esto **debes** programar los siguientes métodos:\n",
    "\n",
    "\n",
    "### **\\_\\_init\\_\\_**\n",
    "Inicializa el algoritmo a partir de: la matriz `X` de los mensajes y la matriz de distancias. Considera además todas las variables que necesites a lo largo de la ejecución de tu algoritmo, se recomienda como mínimo:\n",
    "*   Un contador que indique el nivel actual de aglomeración.\n",
    "*   Un diccionario o lista que almacene los clusters en cada nivel de aglomeración. Inicialmente, en el nivel 0, existe un *cluster* por cada mensaje de `X`.\n",
    "*   La matriz de distancia entre los *clusters* donde el elemento `matriz[id1][id2]` corresponde a la distancia entre los clusters con identificadores `id1` e `id2` respectivamente. \n",
    "*   Una copia de la matriz original X.\n",
    "\n",
    "\n",
    "### **clusterize**\n",
    "Ejecuta el método next_level hasta que solo existan dos *clusters*.\n",
    "\n",
    "##### **next_level**\n",
    "Equivale a realizar un nivel de aglomeración del algoritmo. A modo general deben:\n",
    "1.   Obtener el par de *clusters* con menor distancia a partir de la matriz de distancias obtenida en 3.\n",
    "2.   Unir ambos *clusters*.\n",
    "3.   Guardar el nuevo conjunto de *clusters* correspondientes al nivel actual de aglomeración.\n",
    "4.   Actualizar la matriz de distancias según el nuevo conjunto de *clusters*.\n",
    "\n",
    "##### **update_matrix**\n",
    "Actualiza la matriz de distancias dado un nuevo *cluster*. La distancia entre *clusters* debe poder calcularse según los siguientes enlaces (`linkage`) vistos en clases:\n",
    "1.   **centroid**: distancia entre medias.\n",
    "2.   **single**: simple.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**NOTA**: puedes entregarle los argumentos que quieras a estos métodos y tambien crear otros métodos que consideres pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhMaokeoTulC"
   },
   "source": [
    "En base a lo visto en clases deberás implementar el algoritmo de clustering aglomerativo para agrupar los datos previamente preprocesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMvVAAalUpYb"
   },
   "outputs": [],
   "source": [
    "class AgglomerativeClustering:\n",
    "\n",
    "  def __init__(self, X, linkage=\"centroid\", distance=\"Euclidean\"):\n",
    "    # Vectores de cada pais.\n",
    "    self.X = X.copy()\n",
    "    # Linkage.\n",
    "    self.linkage = linkage\n",
    "    # Métrica de distancia.\n",
    "    self.distance = distance\n",
    "\n",
    "    # Matriz de distancias (obtén la matriz de distancias inicial)\n",
    "    # self.matrix = # \n",
    "\n",
    "    # Copia de la matriz de distancias original.\n",
    "    self.original_matrix = self.matrix.copy()\n",
    "\n",
    "\n",
    "  def clusterize(self):\n",
    "    pass\n",
    "\n",
    "  def next_level(self):\n",
    "    # Obtén el par de clusters con menor distancia de la matriz de distancias.\n",
    "    \n",
    "    # Crea un nuevo cluster a partir de los dos anteriores\n",
    "    \n",
    "    # El nuevo nivel tiene los clusters anteriores y la union de los dos clusters elegidos.\n",
    "  \n",
    "    # Elimina los dos clusters elegidos de la matriz de distancias.\n",
    "    \n",
    "    # Actualiza la matriz de distancias ingresando el nuevo cluster.\n",
    "\n",
    "    pass\n",
    "\n",
    "  \n",
    "  def update_matrix(self, cluster):\n",
    "\n",
    "    if self.linkage == \"centroid\":\n",
    "      pass\n",
    "\n",
    "    elif self.linkage == \"single\":\n",
    "      pass\n",
    "\n",
    "  def get_distance(self, vector1, vector2):\n",
    "    if self.distance == \"Euclidean\": \n",
    "      pass\n",
    "    \n",
    "    elif self.distance == \"Cosine\":\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6ALTMRzXWMB"
   },
   "source": [
    "Utiliza la clase para realizar la aglomeración con los datos preprocesados de spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuhX_nSzXuUx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HPYMJC5X9Vf"
   },
   "source": [
    "# 4. Comparación con distintos parámetros (1 punto)\n",
    "\n",
    "En esta parte deberás comparar distintas configuraciones de tu algoritmo de *clustering* y concluir cual de estas es la mejor.\n",
    "\n",
    "Una forma de comparar *clusters* es a partir de su *silhouette score*. Este mide cuán similar es un objeto a su propio *cluster* (cohesión) en comparación con otros *clusters* (separación). Completa el siguiente código utilizando la función `silhouette_score` de `sklearn.metrics`.\n",
    "\n",
    "NOTA: debes adaptar la estructura de clusters retornada por `AgglomerativeClustering` de tal forma que pueda ser utilizada como los `labels` que recibe `silhouette_score` ([documentación](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD1tB3ZFYHLT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def messages_silhouette_score(X, clusters):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9TmIXKWYUty"
   },
   "source": [
    "Ahora realiza una busqueda de hiperparámetros para encontrar la configuración que retorne el mejor *silhouette_score*. Como mínimo debes probar todas las combinaciones posibles de los siguientes parámetros:\n",
    "\n",
    "*   ***Linkage***: centroid y single.\n",
    "*   ***Distance***: euclidean y cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCVqvki9YjAk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvxjmUzXYonw"
   },
   "source": [
    "Hecho esto, responde las siguientes preguntas. Debes fundamentar todas tus respuestas con los resultados obtenidos en la búsqueda de hiperparametros.\n",
    "1.   ¿Cual configuración fue la mejor? \n",
    "2.   ¿Que métrica de distancia da mejores resultados? (puedes comparar las métricas fijando un valor) \n",
    "3.   ¿Que relación observas entre el método de enlace y la métrica de distancia utilizada? **Justifica**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAcsZZ6-Y4b4"
   },
   "source": [
    "# 5. Visualización (1 punto)\n",
    "\n",
    "Los [dendrogramas](https://es.wikipedia.org/wiki/Dendrograma) son una forma muy útil de visualizar el funcionamiento de los algoritmos de *clustering* aglomerativo. Para completar esta sección debes generar un dendrograma a partir de tu mejor configuración de `AgglomerativeClustering`. Para esto debes utilizar la función `dendrogram` del módulo `cluster.hierarchy` de scipy.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://docs.scipy.org/doc/scipy/_images/scipy-cluster-hierarchy-dendrogram-1_00.png\" width=\"400\"/>\n",
    "\n",
    "Fuente: https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html\n",
    "\n",
    "</center>\n",
    "\n",
    "NOTA: Debes investigar que formato tiene la matriz `Z` (*linkage matrix*) que recibe `dendrogram` y adaptar el output de tu algoritmo acordemente. Está estrictamente prohibido obtener `Z` a partir de la función `linkage` del módulo `cluster.hierarchy`. Puedes modificar la clase `AgglomerativeClustering` si lo consideras necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QLte42CZVCb"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
